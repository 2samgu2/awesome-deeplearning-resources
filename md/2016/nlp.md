## natural language process

- [return to home](../../README.md)

### NLP

- Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwiwufX42tXQAhWCVbwKHZrPBOUQFgguMAE&url=https%3A%2F%2Faclweb.org%2Fanthology%2FK%2FK16%2FK16-1028.pdf&usg=AFQjCNHuJk3k7At-iCnwPJIQEE9GzvOFZg)]
- AC-BLSTM: Asymmetric Convolutional Bidirectional LSTM Networks for Text Classification. [[arxiv](https://arxiv.org/abs/1611.01884)]
- Achieving Human Parity in Conversational Speech Recognition. [[arxiv](https://arxiv.org/abs/1610.05256)]
- A General Framework for Content-enhanced Network Representation Learning. [[arxiv](https://arxiv.org/abs/1610.02906)]
- A Hybrid Geometric Approach for Measuring Similarity Level Among Documents and Document Clustering. [`pdf`](http://ieeexplore.ieee.org/document/7474366/?reload=true) [`code`](https://github.com/taki0112/Vector_Similarity)
- A Joint Many-Task Model- Growing a Neural Network for Multiple NLP Tasks. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwiOjsTEp9PQAhWIwrwKHVeJBcsQFgggMAA&url=https%3A%2F%2Farxiv.org%2Fabs%2F1611.01587&usg=AFQjCNHqixpFo9T8V4ayxskWgKMHkMtTCw)]
- A Semisupervised Approach for Language Identification based on Ladder Networks.  [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&cad=rja&uact=8&ved=0ahUKEwiSmPz52dXQAhUJbbwKHb4BA48QFgguMAI&url=http%3A%2F%2Fwww.eng.biu.ac.il%2Fgoldbej%2Ffiles%2F2012%2F05%2FOdyssey_2016_paper.pdf&usg=AFQjCNGvxKufUzYjNCPDczZkWZ21H4sT-g)]
- A Simple, Fast Diverse Decoding Algorithm for Neural Generation. [[arxiv](https://arxiv.org/abs/1611.08562)]
- Aspect Level Sentiment Classification with Deep Memory Network. [`arxiv`](https://arxiv.org/abs/1605.08900) [`tensorflow`](https://github.com/endymecy/transwarp-nlp) :star:
- A Strong Baseline for Learning Cross-Lingual Word Embeddings from Sentence Alignments. [`arxiv`](https://arxiv.org/abs/1608.05426)
- Cached Long Short-Term Memory Neural Networks for Document-Level Sentiment Classification. [[arxiv](https://arxiv.org/abs/1610.04989)]
- Character-level and Multi-channel Convolutional Neural Networks for Large-scale Authorship Attribution. [[arxiv](https://arxiv.org/abs/1609.06686)]
- COCO-Text-Dataset and Benchmark for Text Detection and Recognition in Natural Images.  [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&cad=rja&uact=8&ved=0ahUKEwjWwraRgdjQAhVFwLwKHUhQCCQQFggxMAI&url=http%3A%2F%2Fsunw.csail.mit.edu%2Fpapers%2F01_Veit_SUNw.pdf&usg=AFQjCNEd0KdDVoHqEmAv4JVphvSIaTG_eg)]
- Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in the Blanks. [[arxiv](https://arxiv.org/abs/1611.00454)]
- Context-aware Natural Language Generation with Recurrent Neural Networks. [[arxiv](https://arxiv.org/abs/1611.09900)]
- <b>[CLSTM]</b> Contextual LSTM models for Large scale NLP tasks.[[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwiQ8O-YgtjQAhXGvrwKHV4OAA4QFggqMAE&url=http%3A%2F%2Fwww.csl.sri.com%2Fusers%2Fshalini%2Fclstm_dlkdd16.pdf&usg=AFQjCNFDWWi_vCjbubOD_XcN-IuQ6uotTQ)] :star: 
- Deep Semi-Supervised Learning with Linguistically Motivated Sequence Labeling Task Hierarchies. [[arxiv](https://arxiv.org/abs/1612.09113)]
- Detecting Text in Natural Image with Connectionist Text Proposal Network. [[arxiv](https://arxiv.org/abs/1609.03605)]
- Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models. [[arxiv](https://arxiv.org/abs/1610.02424)]
- Efficient Character-level Document Classification by Combining Convolution and Recurrent Layers. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwj1yYXdh9jQAhXCebwKHW8vDMMQFggeMAA&url=https%3A%2F%2Farxiv.org%2Fabs%2F1602.00367&usg=AFQjCNF15R9nAUvB5OqWHM2bwLwgrxRPBw)]
- End-to-End Answer Chunk Extraction and Ranking for Reading Comprehension. [[arxiv](https://arxiv.org/abs/1610.09996)]
- End-to-End Multi-View Networks for Text Classification. [`arxiv`](https://arxiv.org/abs/1704.05907)
- End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwjaiLCOiNjQAhWMv7wKHeLQCfsQFggeMAA&url=https%3A%2F%2Farxiv.org%2Fabs%2F1603.01354&usg=AFQjCNG-KfCDJTOPEYjMoohV-fdTGOK9ew)] :star:
- Enhancing and Combining Sequential and Tree LSTM for Natural Language Inference. [[arxiv](https://arxiv.org/abs/1609.06038)]
- Fully Convolutional Instance-aware Semantic Segmentation. [`arxiv`](https://arxiv.org/abs/1611.07709) [`code`](https://github.com/msracver/FCIS)
- Generative Deep Neural Networks for Dialogue: A Short Review. [[arxiv](https://arxiv.org/abs/1611.06216)]
- Generating Factoid Questions With Recurrent Neural Networks- The 30M Factoid Question-Answer Corpus.  [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwjupJy3jNjQAhUCQLwKHTE5AVEQFggnMAE&url=https%3A%2F%2Faclweb.org%2Fanthology%2FP%2FP16%2FP16-1056.pdf&usg=AFQjCNGmLcxTx3Kq2u_yktAPC2XVzpmLzw)] :star: 
- Globally Normalized Transition-Based Neural Networks. [[arxiv](https://arxiv.org/abs/1603.06042)] [[tensorflow](https://github.com/tensorflow/models/tree/master/syntaxnet)] :star:
- GraphNet: Recommendation system based on language and network structure. [`pdf`](https://web.stanford.edu/class/cs224n/reports/2758630.pdf)
- How NOT To Evaluate Your Dialogue System. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwjd0o_ykNjQAhUEgbwKHTSiDR0QFggbMAA&url=https%3A%2F%2Farxiv.org%2Fabs%2F1603.08023&usg=AFQjCNFvRrB0IPgqFO1mhYIsas1dGzdtFQ)] :star: 
- Inducing Multilingual Text Analysis Tools Using Bidirectional Recurrent Neural Networks. [[pdf](https://hal.archives-ouvertes.fr/hal-01374205/document)]
- Key-Value Memory Networks for Directly Reading Documents.  [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwjT98WjktjQAhVDOrwKHfi7CbgQFggmMAE&url=https%3A%2F%2Farxiv.org%2Fpdf%2F1606.03126&usg=AFQjCNEWuqc4X4BJjsozF8U7cxT9RgJXLA)]
- Learning Distributed Representations of Sentences from Unlabelled Data. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwiNu9WMl9jQAhUEi7wKHVSaBsoQFggmMAE&url=https%3A%2F%2Farxiv.org%2Fabs%2F1602.03483&usg=AFQjCNFOfq3lrKNBm8yW1nypxMPW8FpZxQ)] :star: 
- Learning End-to-End Goal-Oriented Dialog. [`arxiv`](https://arxiv.org/abs/1605.07683ï¼‰[`tensorflow`](https://github.com/vyraun/chatbot-MemN2N-tensorflow) :star:
- Learning Recurrent Span Representations for Extractive Question Answering. [[arxiv](https://arxiv.org/abs/1611.01436)]
- Learning to Compose Neural Networks for Question Answering.  [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwj_8fHcl9jQAhWCvLwKHcn7DwQQFggeMAA&url=https%3A%2F%2Farxiv.org%2Fabs%2F1601.01705&usg=AFQjCNGpVsvadnfc-k6tUlbaFXZWCQwzcg)] :star: 
- Learning to Translate in Real-time with Neural Machine Translation. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwjLy53pmNjQAhVDS7wKHbj3CzcQFggeMAA&url=https%3A%2F%2Farxiv.org%2Fabs%2F1610.00388&usg=AFQjCNGgqFSIWXA5ZAO5of1_Opvd1W9OoQ)]
- Linguistically Regularized LSTMs for Sentiment Classification. [[arxiv](https://arxiv.org/abs/1611.03949)]
- Long Short-Term Memory-Networks for Machine Reading. [[url](https://aclweb.org/anthology/D16-1053)] :star: 
- <b>[lda2vec]</b> Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec. [[arxiv](https://arxiv.org/abs/1605.02019)] [[code](https://github.com/cemoody/lda2vec)] [[tensorflow](https://github.com/meereeum/lda2vec-tf)] :star:
- Modeling Coverage for Neural Machine Translation. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&cad=rja&uact=8&ved=0ahUKEwin36mKn9jQAhWIV7wKHeQsDwoQFgg4MAI&url=http%3A%2F%2Fwww.hangli-hl.com%2Fuploads%2F3%2F4%2F4%2F6%2F34465961%2Ftu_et_al_2016.pdf&usg=AFQjCNEUvqmUoV_80qehwowDJxiTKPb56g)] :star: 
- Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss.  [[url](https://www.aclweb.org/anthology/P/P16/P16-2067.pdf)] :star: 
- MultiNet: Real-time Joint Semantic Reasoning for Autonomous Driving. [[arxiv](https://arxiv.org/abs/1612.07695)] :star: 
- Neural Architectures for Fine-grained Entity Type Classification. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwi49K6xodjQAhWKu7wKHVH8C3UQFggeMAA&url=https%3A%2F%2Farxiv.org%2Fabs%2F1606.01341&usg=AFQjCNEBbHYxkxaY1brRzVM-dwjCgxq4RQ)]
- Neural Architectures for Named Entity Recognition. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwi7rMfIodjQAhXCv7wKHXQ-CqQQFgggMAA&url=https%3A%2F%2Farxiv.org%2Fabs%2F1603.01360&usg=AFQjCNEz4-0yI6uDqSCGnCQoDS3FdENHKQ)] :star: 
- Neural Emoji Recommendation in Dialogue Systems.[[arxiv](https://arxiv.org/abs/1612.04609)]
- Neural Paraphrase Generation with Stacked Residual LSTM Networks. [[arxiv](https://arxiv.org/abs/1610.03098)]
- Neural Semantic Encoders. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwjHnrnYo9jQAhVH2LwKHXD9AF0QFggbMAA&url=https%3A%2F%2Farxiv.org%2Fabs%2F1607.04315&usg=AFQjCNFuWivmRnejx165YchYZ6wMsB-snA)]
- Neural Variational Inference for Text Processing. [[arxiv](https://arxiv.org/pdf/1511.06038)] :star: 
- Online Segment to Segment Neural Transduction. [[arxiv](https://arxiv.org/abs/1609.08194)]
- On Random Weights for Texture Generation in One Layer Neural Networks.[[arxiv](https://arxiv.org/abs/1612.06070?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%253A+arxiv%252FQSXk+%2528ExcitingAds%2521+cs+updates+on+arXiv.org%2529)]
- Parallelizing Word2Vec in Shared and Distributed Memory.[[arxiv](https://arxiv.org/abs/1604.04661)] [[github](https://github.com/IntelLabs/pWord2Vec)]
- Phased LSTM: Accelerating Recurrent Network Training for Long or Event-based Sequences. [[arxiv](https://arxiv.org/abs/1610.09513)] [[code](https://github.com/dannyneil/public_plstm)]
- Recurrent Neural Network Grammars. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwiu9uDep9jQAhXEerwKHU3QC_kQFgggMAA&url=https%3A%2F%2Farxiv.org%2Fabs%2F1602.07776&usg=AFQjCNEy_Q-Yep2tn5g00XWwjiGcNgOnrg)] :star: 
- ReasoNet: Learning to Stop Reading in Machine Comprehension. [[arxiv](https://arxiv.org/abs/1609.05284)]
- Sentence Level Recurrent Topic Model- Letting Topics Speak for Themselves.  [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwiEnpWvqtjQAhWEVrwKHYS3B0YQFggeMAA&url=https%3A%2F%2Farxiv.org%2Fabs%2F1604.02038&usg=AFQjCNGkbzfhZME1hdwVrys_l_9pg-L-hA)]
- Sentence-Level Grammatical Error Identification as Sequence-to-Sequence Correction. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwj87-7nqtjQAhUBtZQKHdJiCuUQFggqMAE&url=https%3A%2F%2Faclweb.org%2Fanthology%2FW%2FW16%2FW16-0528.pdf&usg=AFQjCNFB0nh4eIORsJTs4MJ5NdHPCnFaqw)]
- Sentence Ordering using Recurrent Neural Networks. [[arxiv](https://arxiv.org/abs/1611.02654)]
- Sequence to Backward and Forward Sequences: A Content-Introducing Approach to Generative Short-Text Conversation. [[arxiv](https://arxiv.org/abs/1607.00970)]
- Sequential Match Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots. [[arxiv](https://arxiv.org/abs/1612.01627)]
- Structured Sequence Modeling with Graph Convolutional Recurrent Networks.  [[arxiv](https://arxiv.org/abs/1612.07659)]
- [TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency.](http://weibo.com/ttarticle/p/show?id=2309404086416278721142) [[arxiv](https://arxiv.org/abs/1611.01702)]
- Tracking the World State with Recurrent Entity Networks . [[arxiv](https://arxiv.org/abs/1612.03969)] :star: 
- Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM Encoder-Decoder.[[arxiv](https://arxiv.org/abs/1607.07514)] [[code](https://github.com/soroushv/Tweet2Vec)]
- Unsupervised Learning of Sentence Representations using Convolutional Neural Networks. [[url](https://arxiv.org/abs/1611.07897)]
- Unsupervised neural and Bayesian models for zero-resource speech processing. [[arxiv](https://arxiv.org/abs/1701.00851)]
- Unsupervised Pretraining for Sequence to Sequence Learning. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwidp53EsdjQAhVMzbwKHeSfBa4QFggjMAA&url=https%3A%2F%2Farxiv.org%2Fabs%2F1611.02683&usg=AFQjCNHdHMJUM2OIgLMaZs5wpbXfXvN4gA)]
- UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text. [[arxiv](https://arxiv.org/abs/1611.03599)]
- Very Deep Convolutional Networks for Natural Language Processing.  [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwjj79_ZsdjQAhVJy7wKHRLdAJEQFggsMAE&url=https%3A%2F%2Farxiv.org%2Fpdf%2F1606.01781&usg=AFQjCNEX8WGvkSXZgPzlKLelkfkhlC2Tnw)] :star: 
- Visual Dialog. [`arxiv`](https://arxiv.org/abs/1611.08669) [`code`](https://github.com/Cloud-CV/visual-chatbot) :star:
- [Wide & Deep Learning for Recommender Systems.](http://blog.csdn.net/dinosoft/article/details/52581368) [[arxiv](https://arxiv.org/abs/1606.07792)] [[tensorflow](https://www.tensorflow.org/tutorials/wide_and_deep/)]:star:

### Generative learning

- Adversarial Training Methods for Semi-Supervised Text Classification. [[arxiv](https://arxiv.org/abs/1605.07725)]
- Aspect Level Sentiment Classification with Deep Memory Network. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwja0LeI3tXQAhXMgLwKHZ83A04QFgguMAE&url=https%3A%2F%2Farxiv.org%2Fpdf%2F1605.08900&usg=AFQjCNGfnoyCcCzGnIDOhLeYky4UdUa2OQ)]
- Generative Adversarial Text to Image Synthesis. [[arxiv](https://arxiv.org/abs/1605.05396)] :star: 
- Modeling documents with Generative Adversarial Networks. [[arxiv](https://arxiv.org/abs/1612.09122)]
- <b>[StackGAN]</b> StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks. [[url](https://arxiv.org/abs/1612.03242)] [[code](https://github.com/hanzhanggit/StackGAN)] :star:

### Attention and memory

- A Context-aware Attention Network for Interactive Question Answering. [`url`](https://openreview.net/pdf?id=SkyQWDcex)
- A Decomposable Attention Model for Natural Language Inference. [[arxiv](https://arxiv.org/abs/1606.01933)] [[code](https://github.com/harvardnlp/decomp-attn)]
- A self-attentive sentence embedding.[[url](https://www.google.com.hk/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwjv-M3H0Y_RAhVS0mMKHYz3BAsQFggeMAA&url=https%3A%2F%2Fopenreview.net%2Fpdf%3Fid%3DBJC_jUqxe&usg=AFQjCNGzzX7jxAZ3tYtd7t4LlE6S7KNkpQ)]
- Aspect Level Sentiment Classification with Deep Memory Network. [[arxiv](https://arxiv.org/abs/1605.08900)] [[code](http://nlp.stanford.edu/sentiment/code.html)] :star:
- AttSum: Joint Learning of Focusing and Summarization with Neural Attention. [[arxiv](https://arxiv.org/abs/1604.00125)]
- Attention-over-Attention Neural Networks for Reading Comprehension.  [[arxiv](https://arxiv.org/abs/1607.04423)] [[github](https://www.google.com.hk/url?sa=t&rct=j&q=&esrc=s&source=web&cd=9&cad=rja&uact=8&ved=0ahUKEwih0JvJpZTRAhUijlQKHQlBAaQQFghIMAg&url=https%3A%2F%2Fgithub.com%2FOlavHN%2Fattention-over-attention&usg=AFQjCNG-i1RrT0JFwPhik65XukwXwy9ljw)]
- Collective Entity Resolution with Multi-Focal Attention.  [[aclweb](https://www.aclweb.org/anthology/P/P16/P16-1059.pdf)]
- [Gated-Attention Readers for Text Comprehension.](https://theneuralperspective.com/2017/01/19/gated-attention-readers-for-text-comprehension/)  [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwi03LiIjNjQAhUFO7wKHXUmAE4QFggnMAE&url=https%3A%2F%2Fopenreview.net%2Fpdf%3Fid%3DHkcdHtqlx&usg=AFQjCNF8nqsWtlgBgepZz-U1diL7mcGaYg)]
- Hierarchical Attention Networks for Document Classification. [`pdf`](https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf) [`tensorflow`](https://github.com/ematvey/deep-text-classifier) :star: 
- Hierarchical Memory Networks for Answer Selection on Unknown Words. [[arxiv](https://arxiv.org/abs/1609.08843)]
- Implicit Distortion and Fertility Models for Attention-based Encoder-Decoder NMT Model. [[arxiv](https://arxiv.org/abs/1601.03317)]
- Improving Attention Modeling with Implicit Distortion and Fertility for Machine Translation. [[pdf](https://www.aclweb.org/anthology/C/C16/C16-1290.pdf)]
- Iterative Alternating Neural Attention for Machine Reading. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwjt79qFktjQAhVIvrwKHe5cCGgQFggrMAE&url=https%3A%2F%2Farxiv.org%2Fpdf%2F1606.02245&usg=AFQjCNF3I_gZskJ890hJzpm2_3yIUVeEsg)]
- Joint CTC-Attention based End-to-End Speech Recognition using Multi-task Learning. [[arxiv](https://arxiv.org/abs/1609.06773)]
- Key-Value Memory Networks for Directly Reading Documents. [[arxiv](https://arxiv.org/abs/1606.03126)]
- Knowledge as a Teacher: Knowledge-Guided Structural Attention Networks. [[arxiv](https://arxiv.org/abs/1609.03286)]
- Language to Logical Form with Neural Attention.  [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&cad=rja&uact=8&ved=0ahUKEwjlhojaldjQAhVGfLwKHYfZAaEQFggvMAI&url=http%3A%2F%2Fhomepages.inf.ed.ac.uk%2Fs1478528%2Facl16-lang2logic-slides.pdf&usg=AFQjCNFfAVG7Xp0RYOo1H4AplPhTvKCayQ)] :star: 
- Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwjX8eG_l9jQAhVIULwKHawzDbIQFggeMAA&url=https%3A%2F%2Farxiv.org%2Fabs%2F1605.09090&usg=AFQjCNF_8Q0p5EdAfY1k8YOWG0gD8MKJ0A)]
- Lexicon Integrated CNN Models with Attention for Sentiment Analysis. [[arxiv](https://arxiv.org/abs/1610.06272)]
- Memory-enhanced Decoder for Neural Machine Translation. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwjo_ezwntjQAhUBvbwKHdrrBzkQFggfMAA&url=https%3A%2F%2Farxiv.org%2Fabs%2F1606.02003&usg=AFQjCNFi6ZffMp9CIjAr3oWHtfZCP5YpCg)]
- Neural Language Correction with Character-Based Attention.  [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwjBmYDwodjQAhXMTLwKHb_HB5sQFggeMAA&url=https%3A%2F%2Farxiv.org%2Fabs%2F1603.09727&usg=AFQjCNHJRTjauP0qBRF20-J6Qpzq1Odxdw)]
- Visualizing and Understanding Curriculum Learning for Long Short-Term Memory Networks. [[arxiv](https://arxiv.org/abs/1611.06204)]

### Neural Machine Translation

- Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models. [[arxiv](https://arxiv.org/abs/1604.00788)] :star: 
- A Character-level Decoder without Explicit Segmentation for Neural Machine Translation.  [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwiZr6_ZpdPQAhWEu7wKHT_1AJ4QFggpMAE&url=https%3A%2F%2Fwww.aclweb.org%2Fanthology%2FP%2FP16%2FP16-1160.pdf&usg=AFQjCNGNp_ng6FPcHatCYdgRC_jIsiufkg)] :star: 
- Character-based Neural Machine Translation. [[arxiv](https://arxiv.org/abs/1511.04586)] :star: 
- Context-Dependent Word Representation for Neural Machine Translation. [[arxiv](https://arxiv.org/pdf/1607.00578.pdf)]
- Convolutional Encoders for Neural Machine Translation. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwiRztLsgtjQAhUJybwKHYZNBI8QFggfMAA&url=https%3A%2F%2Fcs224d.stanford.edu%2Freports%2FLambAndrew.pdf&usg=AFQjCNGc36jpBqQdaKI19b67nIEnkaZmxw)]
- Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translatin. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwjs08KahtjQAhVNQLwKHV8_DC0QFggmMAE&url=https%3A%2F%2Farxiv.org%2Fpdf%2F1606.04199&usg=AFQjCNGE9o3aCzRNtLcwaKfUjO7FV2gYaA)]
- Dual Learning for Machine Translation. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwjpoa3dhtjQAhWJTLwKHWr0DqYQFggrMAE&url=https%3A%2F%2Fpapers.nips.cc%2Fpaper%2F6469-dual-learning-for-machine-translation.pdf&usg=AFQjCNHMsJHcP9wBymQ7yFPMn8P_34nzfA)]
- Fast Domain Adaptation for Neural Machine Translation. [[arxiv](https://arxiv.org/abs/1612.06897)]
- Fully Character-Level Neural Machine Translation without Explicit Segmentation. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&cad=rja&uact=8&ved=0ahUKEwjzm5bji9jQAhWKXbwKHY7XDq4QFggyMAI&url=https%3A%2F%2Farxiv.org%2Fpdf%2F1610.03017&usg=AFQjCNHnbc8-GslAad2RBzmrV2ppgRsbmQ)] :star: 
- Google's Multilingual Neural Machine Translation System- Enabling Zero-Shot Translation.  [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwiwnYPSjdjQAhXMvLwKHfAdCwkQFggjMAE&url=https%3A%2F%2Farxiv.org%2Fabs%2F1611.04558&usg=AFQjCNEOkgAI_1Cj_4LoU6pZjGj9s9VdMA)]
- Google's Neural Machine Translation System- Bridging the Gap between Human and Machine Translation.  [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwio0qP2jdjQAhUHvrwKHXuxCiIQFgghMAA&url=https%3A%2F%2Farxiv.org%2Fabs%2F1609.08144&usg=AFQjCNHrQteiCIO8woQ1piRonQeZbYaYtw)] :star: 
- How Grammatical is Character-level Neural Machine Translation? Assessing MT Quality with Contrastive Translation Pairs.[[arxiv](https://arxiv.org/abs/1612.04629)]
- Interactive Attention for Neural Machine Translation. [[arxiv](https://arxiv.org/abs/1610.05011)]
- Multimodal Attention for Neural Machine Translation. [[arxiv](https://arxiv.org/abs/1609.03976)]
- Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwibwPzrn9jQAhWEWLwKHYjDAOwQFggnMAE&url=https%3A%2F%2Farxiv.org%2Fpdf%2F1601.01073&usg=AFQjCNGBCM_nN20wGuG4LsZX_0F5CBfLvQ)] :star: 
- Modeling Coverage for Neural Machine Translation. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&cad=rja&uact=8&ved=0ahUKEwin36mKn9jQAhWIV7wKHeQsDwoQFgg4MAI&url=http%3A%2F%2Fwww.hangli-hl.com%2Fuploads%2F3%2F4%2F4%2F6%2F34465961%2Ftu_et_al_2016.pdf&usg=AFQjCNEUvqmUoV_80qehwowDJxiTKPb56g)] :star: 
- Neural Machine Translation in Linear Time. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwjn5tGIotjQAhWKiLwKHUfSDtsQFggiMAA&url=https%3A%2F%2Farxiv.org%2Fabs%2F1610.10099&usg=AFQjCNHrnVpbFg6yFqb238lgScLZOEcISw)]
- Neural Network Translation Models for Grammatical Error Correction. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwi4pcSMo9jQAhULT7wKHZmlBlQQFggvMAA&url=https%3A%2F%2Farxiv.org%2Fpdf%2F1606.00189&usg=AFQjCNH4q8_JVt6qRe-Gmlwlr1dz-ugqtA)]
- Neural Machine Translation with Latent Semantic of Image and Text. [[arxiv](https://arxiv.org/abs/1611.08459)]
- Neural Machine Translation with Pivot Languages. [[arxiv](https://arxiv.org/abs/1611.04928)]
- Neural Machine Translation with Recurrent Attention Modeling. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwjhkfGlotjQAhXFULwKHTzhA1YQFggbMAA&url=https%3A%2F%2Farxiv.org%2Fabs%2F1607.05108&usg=AFQjCNFLr_lvHbiSPwL3pP5mq0EHdGZiDA)]
- Neural Machine Translation with Supervised Attention. [[pdf](https://www.aclweb.org/anthology/C/C16/C16-1291.pdf)]
- Recurrent Neural Machine Translation. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwjbzcDKp9jQAhVMzLwKHZ2FDqIQFggbMAA&url=https%3A%2F%2Farxiv.org%2Fabs%2F1607.08725&usg=AFQjCNEl13PMYPOwO2mTcCK_bdwUNFTdNQ)]
- Semi-Supervised Learning for Neural Machine Translation. [[pdf](http://iiis.tsinghua.edu.cn/~weixu/files/acl2016_chengyong.pdf)]
- Temporal Attention Model for Neural Machine Translation. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwiQipLur9jQAhXMS7wKHU2dC6IQFgggMAA&url=https%3A%2F%2Farxiv.org%2Fabs%2F1608.02927&usg=AFQjCNFbmrm7D9W3GN1Luapp-sRVHqKlKA)]
- Zero-Resource Translation with Multi-Lingual Neural Machine Translation. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwixt5-ks9jQAhWBiLwKHTcaC_oQFgguMAE&url=https%3A%2F%2Farxiv.org%2Fpdf%2F1606.04164&usg=AFQjCNE8wtAunVCjcDjilk5cyovuj_zlYA)]

### Neural Language Model

- Character-Aware Neural Language Models. [[pdf](https://www.google.com.hk/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwia54TEqJTRAhVrw1QKHflTBOEQFggpMAE&url=%68%74%74%70%3a%2f%2f%77%77%77%2e%61%61%61%69%2e%6f%72%67%2f%6f%63%73%2f%69%6e%64%65%78%2e%70%68%70%2f%41%41%41%49%2f%41%41%41%49%31%36%2f%70%61%70%65%72%2f%64%6f%77%6e%6c%6f%61%64%2f%31%32%34%38%39%2f%31%32%30%31%37&usg=AFQjCNHeBBKDjQEfBu4aDgG8zUkd5Y1tsA)] :star: 
- Character-Level Language Modeling with Hierarchical Recurrent Neural Networks. [[arxiv](https://arxiv.org/abs/1609.03777)]
- Coherent Dialogue with Attention-based Language Models. [[arxiv](https://arxiv.org/abs/1611.06997)]
- Joint Online Spoken Language Understanding and Language Modeling with Recurrent Neural Networks. [[arxiv](https://arxiv.org/abs/1609.01462)]
- Improving neural language models with a continuous cache.[[url](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwiAp7G86IvRAhXmiFQKHXr2CXcQFggiMAA&url=https%3A%2F%2Fopenreview.net%2Fpdf%3Fid%3DB184E5qee&usg=AFQjCNEJ76Q5GPhvUBE_gOmo7_spiWbbmQ)]
- Language Modeling with Gated Convolutional Networks.[[arxiv](https://arxiv.org/abs/1612.08083)] [[tensorflow](https://github.com/anantzoid/Language-Modeling-GatedCNN)] :star: 
- Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling. [[arxiv](https://arxiv.org/abs/1611.08034)]
- Recurrent Memory Networks for Language Modeling. [[url](https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwiM-uaop9jQAhWEjLwKHacvB_oQFggsMAE&url=https%3A%2F%2Farxiv.org%2Fpdf%2F1601.01272&usg=AFQjCNE1W-90ZYxVaCls2sBch5JuzPbVcA)]
