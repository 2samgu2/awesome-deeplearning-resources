## nature language process

### deep learning

- A Comparative Study of Word Embeddings for Reading Comprehension. [`arxiv`](https://arxiv.org/abs/1703.00993)
- A Tidy Data Model for Natural Language Processing using cleanNLP. [`arXiv`](https://arxiv.org/abs/1703.09570)
- Autoencoding Variational Inference For Topic Models. [`arxiv`](https://arxiv.org/abs/1703.01488) [`tensorflow`](https://github.com/akashgit/autoencoding_vi_for_topic_models) :star:
- Automatic Rule Extraction from Long Short Term Memory Networks. [`arxiv`](https://arxiv.org/abs/1702.02540)
- Billion-scale similarity search with GPUs. [`arXiv`](https://arxiv.org/abs/1702.08734) [`code`](https://github.com/facebookresearch/faiss) :star:
- Deep Recurrent Neural Network for Protein Function Prediction from Sequence. [`arxiv`](https://arxiv.org/abs/1701.08318)
- Deep Voice: Real-time Neural Text-to-Speech. [`arxiv`](https://arxiv.org/abs/1702.07825) :star:
- Dialog Context Language Modeling with Recurrent Neural Networks. [`arxiv`](https://arxiv.org/abs/1701.04056)
- dna2vec: Consistent vector representations of variable-length k-mers. [`arxiv`](https://arxiv.org/abs/1701.06279) [`code`](https://pnpnpn.github.io/dna2vec/)
- Efficient Vector Representation For Documents Through Corruption. [`pdf`](https://openreview.net/pdf?id=B1Igu2ogg)] [`code`](https://github.com/mchen24/iclr2017) :star:
- End-to-End MAP Training of a Hybrid HMM-DNN Model. [`arXiv`](https://arxiv.org/abs/1703.10356)
- End-to-end semantic face segmentation with conditional random fields as convolutional, recurrent and adversarial networks. [`arxiv`](https://arxiv.org/abs/1703.03305)
- End-to-End Task-Completion Neural Dialogue Systems. [`arxiv`](https://arxiv.org/abs/1703.01008) [`code`](https://github.com/MiuLab/TC-Bot)] :star:
- Joint Semantic Synthesis and Morphological Analysis of the Derived Word. [`arxiv`](https://arxiv.org/abs/1701.00946)
- High-Throughput and Language-Agnostic Entity Disambiguation and Linking on User Generated Data. [`arxiv`](https://arxiv.org/abs/1703.04498)
- Label Refinement Network for Coarse-to-Fine Semantic Segmentation. [`arxiv`](https://arxiv.org/abs/1703.00551)
- LanideNN: Multilingual Language Identification on Character Window. [`arxiv`](https://arxiv.org/abs/1701.03338) [`code`](https://github.com/tomkocmi/LanideNN)
- Learning a Natural Language Interface with Neural Programmer. [`arxiv`](https://arxiv.org/abs/1611.08945) [`tensorflow`](https://github.com/tensorflow/models/tree/master/neural_programmer) :star:
- Learning Arbitrary Potentials in CRFs with Gradient Descent. [`arxiv`](https://arxiv.org/abs/1701.06805)
- Multi-level Representations for Fine-Grained Typing of Knowledge Base Entities. [`arxiv`](https://arxiv.org/abs/1701.02025)
- Neural Probabilistic Model for Non-projective MST Parsing. [`arxiv`](https://arxiv.org/abs/1701.00874)
- N-gram Language Modeling using Recurrent Neural Network Estimation. [`arxiv`](https://arxiv.org/abs/1703.10724) :star:
- One Representation per Word - Does it make Sense for Composition?.  [`arxiv`](https://arxiv.org/abs/1702.06696)
- Outlier Detection for Text Data : An Extended Version. [`arxiv`](https://128.84.21.199/abs/1701.01325v1)
- Person Search with Natural Language Description. [`arxiv`](https://arxiv.org/abs/1702.05729)
- Question Answering from Unstructured Text by Retrieval and Comprehension. [`arxiv`](https://arxiv.org/abs/1703.08885)
- Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations. [`arxiv`](https://arxiv.org/abs/1702.07826) :star:
- Recurrent and Contextual Models for Visual Question Answering. [`arxiv`](https://arxiv.org/abs/1703.08120)
- Recurrent Recommender Networks. [`pdf`](http://alexbeutel.com/papers/rrn_wsdm2017.pdf)
- Symbolic, Distributed and Distributional Representations for Natural Language Processing in the Era of Deep Learning: a Survey. [`arxiv`](https://arxiv.org/abs/1702.00764)
- Tacotron: A Fully End-to-End Text-To-Speech Synthesis Model. [`arxiv`](https://arxiv.org/abs/1703.10135) [`code`](https://github.com/google/tacotron)
- Twitter100k: A Real-world Dataset for Weakly Supervised Cross-Media Retrieval. [`arxiv`](https://arxiv.org/abs/1703.06618) [`code`](https://github.com/huyt16/Twitter100k/)
- Understanding Convolution for Semantic Segmentation. [`arxiv`](https://arxiv.org/abs/1702.08502)
- Vector Embedding of Wikipedia Concepts and Entities. [`arxiv`](https://arxiv.org/abs/1702.03470) [`code`](https://github.com/ehsansherkat/ConVec)]
- VINet: Visual-Inertial Odometry as a Sequence-to-Sequence Learning Problem. [`arxiv`](https://arxiv.org/abs/1701.08376)

### Attention and memory 

- End-to-End Attention based Text-Dependent Speaker Verification. [`arxiv`](https://arxiv.org/abs/1701.00562)
- Frustratingly Short Attention Spans in Neural Language Modeling. [`arxiv`](https://arxiv.org/abs/1702.04521)
- Structural Attention Neural Networks for improved sentiment analysis. [`arxiv`](https://arxiv.org/abs/1701.01811)

### Generative learning

- [Adversarial Learning for Neural Dialogue Generation.](https://zhuanlan.zhihu.com/p/25027693) [`arXiv`](https://arxiv.org/abs/1701.06547) [`tensorflow`](https://github.com/BigPlay/Adversarial-Learning-for-Neural-Dialogue-Generation-in-Tensorflow)
- Improved Variational Autoencoders for Text Modeling using Dilated Convolutions. [`arxiv`](https://arxiv.org/abs/1702.08139)
  
### Transfer learning

- Domain Adaptation in Question Answering. [`arxiv`](https://arxiv.org/abs/1702.02171)
- Neural Machine Translation and Sequence-to-sequence Models: A Tutorial. [`arxiv`](https://arxiv.org/abs/1703.01619)
- Transfer Learning for Low-Resource Chinese Word Segmentation with a Novel Neural Network. [`arxiv`](https://arxiv.org/abs/1702.04488) [`code`](https://github.com/jincy520/Low-Resource-CWS-)]