## nature language process

### deep learning

- A Comparative Study of Word Embeddings for Reading Comprehension. [[arXiv](https://arxiv.org/abs/1703.00993)]
- A Tidy Data Model for Natural Language Processing using cleanNLP. [`arXiv`](https://arxiv.org/abs/1703.09570)
- Autoencoding Variational Inference For Topic Models. [[arXiv](https://arxiv.org/abs/1703.01488)] [[tensorflow](https://github.com/akashgit/autoencoding_vi_for_topic_models)] :star:
- Automatic Rule Extraction from Long Short Term Memory Networks. [[arXiv](https://arxiv.org/abs/1702.02540)]
- Deep Recurrent Neural Network for Protein Function Prediction from Sequence. [[arXiv](https://arxiv.org/abs/1701.08318)]
- Deep Voice: Real-time Neural Text-to-Speech. [[arXiv](https://arxiv.org/abs/1702.07825)] :star:
- Dialog Context Language Modeling with Recurrent Neural Networks. [[arXiv](https://arxiv.org/abs/1701.04056)]
- dna2vec: Consistent vector representations of variable-length k-mers. [[arXiv](https://arxiv.org/abs/1701.06279)] [[code](https://pnpnpn.github.io/dna2vec/)]
- Efficient Vector Representation For Documents Through Corruption. [[pdf](https://openreview.net/pdf?id=B1Igu2ogg)] [[code](https://github.com/mchen24/iclr2017)] :star:
- End-to-end semantic face segmentation with conditional random fields as convolutional, recurrent and adversarial networks. [[arXiv](https://arxiv.org/abs/1703.03305)]
- End-to-End Task-Completion Neural Dialogue Systems. [[arXiv](https://arxiv.org/abs/1703.01008)] [[code](https://github.com/MiuLab/TC-Bot)] :star:
- Joint Semantic Synthesis and Morphological Analysis of the Derived Word. [[arXiv](https://arxiv.org/abs/1701.00946)]
- High-Throughput and Language-Agnostic Entity Disambiguation and Linking on User Generated Data. [[arXiv](https://arxiv.org/abs/1703.04498)]
- Label Refinement Network for Coarse-to-Fine Semantic Segmentation. [[arXiv](https://arxiv.org/abs/1703.00551)]
- LanideNN: Multilingual Language Identification on Character Window. [[arXiv](https://arxiv.org/abs/1701.03338)] [[code](https://github.com/tomkocmi/LanideNN)]
- Learning a Natural Language Interface with Neural Programmer. [[arXiv](https://arxiv.org/abs/1611.08945)] [[tensorflow](https://github.com/tensorflow/models/tree/master/neural_programmer)] :star:
- Learning Arbitrary Potentials in CRFs with Gradient Descent. [[arXiv](https://arxiv.org/abs/1701.06805)]
- Multi-level Representations for Fine-Grained Typing of Knowledge Base Entities. [[arXiv](https://arxiv.org/abs/1701.02025)]
- Neural Probabilistic Model for Non-projective MST Parsing. [[arXiv](https://arxiv.org/abs/1701.00874)]
- One Representation per Word - Does it make Sense for Composition?.  [[arXiv](https://arxiv.org/abs/1702.06696)]
- Outlier Detection for Text Data : An Extended Version. [[arXiv](https://128.84.21.199/abs/1701.01325v1)]
- Person Search with Natural Language Description. [[arXiv](https://arxiv.org/abs/1702.05729)]
- Question Answering from Unstructured Text by Retrieval and Comprehension. [`arxiv`](https://arxiv.org/abs/1703.08885)
- Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations. [[arXiv](https://arxiv.org/abs/1702.07826)] :star:
- Recurrent and Contextual Models for Visual Question Answering. [`arxiv`](https://arxiv.org/abs/1703.08120)
- Recurrent Recommender Networks. [`pdf`](http://alexbeutel.com/papers/rrn_wsdm2017.pdf)
- Symbolic, Distributed and Distributional Representations for Natural Language Processing in the Era of Deep Learning: a Survey. [[arXiv](https://arxiv.org/abs/1702.00764)]
- Tacotron: A Fully End-to-End Text-To-Speech Synthesis Model. [`arxiv`](https://arxiv.org/abs/1703.10135) [`code`](https://github.com/google/tacotron)
- Twitter100k: A Real-world Dataset for Weakly Supervised Cross-Media Retrieval. [`arxiv`](https://arxiv.org/abs/1703.06618) [`code`](https://github.com/huyt16/Twitter100k/)
- Understanding Convolution for Semantic Segmentation. [[arXiv](https://arxiv.org/abs/1702.08502)]
- Vector Embedding of Wikipedia Concepts and Entities. [[arXiv](https://arxiv.org/abs/1702.03470)] [[code](https://github.com/ehsansherkat/ConVec)]
- VINet: Visual-Inertial Odometry as a Sequence-to-Sequence Learning Problem. [[arXiv](https://arxiv.org/abs/1701.08376)]

### Attention and memory 

- End-to-End Attention based Text-Dependent Speaker Verification. [[arXiv](https://arxiv.org/abs/1701.00562)]
- Frustratingly Short Attention Spans in Neural Language Modeling. [[arXiv](https://arxiv.org/abs/1702.04521)]
- Structural Attention Neural Networks for improved sentiment analysis. [[arXiv](https://arxiv.org/abs/1701.01811)]

### Generative learning

- [Adversarial Learning for Neural Dialogue Generation.](https://zhuanlan.zhihu.com/p/25027693) [[arXiv](https://arxiv.org/abs/1701.06547)]
- Improved Variational Autoencoders for Text Modeling using Dilated Convolutions. [[arXiv](https://arxiv.org/abs/1702.08139)] 
  
### Transfer learning

- Domain Adaptation in Question Answering. [[arXiv](https://arxiv.org/abs/1702.02171)]
- Neural Machine Translation and Sequence-to-sequence Models: A Tutorial. [[arXiv](https://arxiv.org/abs/1703.01619)]
- Transfer Learning for Low-Resource Chinese Word Segmentation with a Novel Neural Network. [[arXiv](https://arxiv.org/abs/1702.04488)] [[code](https://github.com/jincy520/Low-Resource-CWS-)]